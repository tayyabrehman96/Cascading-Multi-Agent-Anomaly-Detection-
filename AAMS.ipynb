{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tayyabrehman96/Cascading-Multi-Agent-Anomaly-Detection-/blob/main/AAMS.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ViQcX8T45ZUy"
      },
      "outputs": [],
      "source": [
        "# Import the necessary library\n",
        "from google.colab import drive\n",
        "\n",
        "# Mount your Google Drive\n",
        "# This will prompt you for authorization. Follow the link, get the code,\n",
        "# and paste it back in the input box.\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "print(\"\\n‚úÖ Google Drive has been successfully mounted!\")\n",
        "print(\"You can now browse your files in the folder icon on the left.\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os, zipfile, glob\n",
        "\n",
        "# --- Path to your ZIP file in Google Drive ---\n",
        "zip_path = \"/content/drive/MyDrive/Dataset Conf/archive.zip\"\n",
        "extract_path = \"/content/local_dataset\"\n",
        "\n",
        "# --- Step 1: Check if ZIP file exists ---\n",
        "if not os.path.isfile(zip_path):\n",
        "    print(f\"\\n‚ùå ERROR: ZIP file not found at: {zip_path}\")\n",
        "    print(\"\\nTip: Check spelling, spacing (e.g., 'Dataset Conf'), and right-click ‚Üí Copy path in Colab.\")\n",
        "else:\n",
        "    print(f\"\\n‚úÖ Found ZIP file: {zip_path}\")\n",
        "\n",
        "    # --- Step 2: Extract if not already extracted ---\n",
        "    if not os.path.exists(extract_path):\n",
        "        print(\"‚è≥ Extracting ZIP... this may take a while.\")\n",
        "        os.makedirs(extract_path, exist_ok=True)\n",
        "        with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "            zip_ref.extractall(extract_path)\n",
        "        print(\"‚úÖ Extraction complete!\")\n",
        "    else:\n",
        "        print(\"‚ö° Already extracted, skipping extraction.\")\n",
        "\n",
        "    # --- Step 3: Collect ONLY image files ---\n",
        "    image_extensions = [\"*.jpg\", \"*.jpeg\", \"*.png\"]\n",
        "    all_images = []\n",
        "    for ext in image_extensions:\n",
        "        all_images.extend(glob.glob(os.path.join(extract_path, \"**\", ext), recursive=True))\n",
        "\n",
        "    print(\"-\" * 40)\n",
        "    print(f\"üìÇ Extraction folder: {extract_path}\")\n",
        "    print(f\"üì∏ Total images found: {len(all_images):,}\")\n",
        "    if all_images:\n",
        "        print(f\"üëâ Example image: {all_images[0]}\")\n",
        "    print(\"-\" * 40)\n"
      ],
      "metadata": {
        "id": "lgEe7OO46QGj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 1. Imports ---\n",
        "import os, glob, shutil\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tqdm import tqdm\n",
        "\n",
        "# --- 2. Force GPU (Tesla T4) ---\n",
        "assert torch.cuda.is_available(), \"‚ùå No GPU found! Please enable GPU runtime in Colab.\"\n",
        "device = torch.device(\"cuda\")\n",
        "print(f\"üî• Using device: {device}\")\n",
        "print(\"GPU Name:\", torch.cuda.get_device_name(0))\n",
        "\n",
        "# --- 3. Path after extraction ---\n",
        "DATASET_PATH = \"/content/local_dataset\"\n",
        "PROCESSED_PATH = \"/content/processed_dataset\"\n",
        "\n",
        "# --- 4. Collect all images (.jpg, .jpeg, .png) ---\n",
        "image_extensions = [\"*.jpg\", \"*.jpeg\", \"*.png\"]\n",
        "all_images = []\n",
        "for ext in image_extensions:\n",
        "    all_images.extend(glob.glob(os.path.join(DATASET_PATH, \"**\", ext), recursive=True))\n",
        "\n",
        "print(f\"üì∏ Total images collected: {len(all_images):,}\")\n",
        "if not all_images:\n",
        "    raise RuntimeError(\"‚ùå No images found! Check dataset extraction.\")\n",
        "\n",
        "# --- 5. Train/Val/Test split ---\n",
        "train_imgs, temp_imgs = train_test_split(all_images, test_size=0.2, random_state=42)\n",
        "val_imgs, test_imgs = train_test_split(temp_imgs, test_size=0.5, random_state=42)\n",
        "\n",
        "print(f\"üìÇ Train: {len(train_imgs):,}, Val: {len(val_imgs):,}, Test: {len(test_imgs):,}\")\n",
        "\n",
        "# --- 6. Organize into folders ---\n",
        "for split in [\"train\", \"val\", \"test\"]:\n",
        "    os.makedirs(os.path.join(PROCESSED_PATH, split), exist_ok=True)\n",
        "\n",
        "def copy_images(file_list, destination):\n",
        "    for f in file_list:\n",
        "        shutil.copy(f, destination)\n",
        "\n",
        "copy_images(train_imgs, os.path.join(PROCESSED_PATH, \"train\"))\n",
        "copy_images(val_imgs, os.path.join(PROCESSED_PATH, \"val\"))\n",
        "copy_images(test_imgs, os.path.join(PROCESSED_PATH, \"test\"))\n",
        "print(\"‚úÖ Dataset organized into train/val/test folders.\")\n",
        "\n",
        "# --- 7. Custom Dataset ---\n",
        "class ImageDataset(Dataset):\n",
        "    def __init__(self, folder_path, transform=None):\n",
        "        self.image_paths = sorted(glob.glob(os.path.join(folder_path, \"*\")))\n",
        "        self.transform = transform\n",
        "    def __len__(self):\n",
        "        return len(self.image_paths)\n",
        "    def __getitem__(self, idx):\n",
        "        image = Image.open(self.image_paths[idx]).convert(\"RGB\")\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        return image\n",
        "\n",
        "# --- 8. Data transforms & loaders ---\n",
        "IMAGE_SIZE = (128, 128)\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize(IMAGE_SIZE),\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "\n",
        "train_dataset = ImageDataset(os.path.join(PROCESSED_PATH, \"train\"), transform=transform)\n",
        "val_dataset = ImageDataset(os.path.join(PROCESSED_PATH, \"val\"), transform=transform)\n",
        "test_dataset = ImageDataset(os.path.join(PROCESSED_PATH, \"test\"), transform=transform)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True,\n",
        "                          num_workers=4, pin_memory=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False,\n",
        "                        num_workers=4, pin_memory=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False,\n",
        "                         num_workers=4, pin_memory=True)\n",
        "\n",
        "print(f\"‚úÖ Dataset ready: {len(train_dataset)} train, {len(val_dataset)} val, {len(test_dataset)} test\")\n",
        "\n",
        "# --- 9. Autoencoder model (Contraction + Reconstruction) ---\n",
        "class ConvAutoencoder(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(ConvAutoencoder, self).__init__()\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Conv2d(3, 32, 3, stride=2, padding=1), nn.ReLU(),\n",
        "            nn.Conv2d(32, 64, 3, stride=2, padding=1), nn.ReLU(),\n",
        "            nn.Conv2d(64, 128, 3, stride=2, padding=1), nn.ReLU(),\n",
        "        )\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.ConvTranspose2d(128, 64, 3, stride=2, padding=1, output_padding=1), nn.ReLU(),\n",
        "            nn.ConvTranspose2d(64, 32, 3, stride=2, padding=1, output_padding=1), nn.ReLU(),\n",
        "            nn.ConvTranspose2d(32, 3, 3, stride=2, padding=1, output_padding=1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        return self.decoder(self.encoder(x))\n",
        "\n",
        "# --- 10. Init model, loss, optimizer ---\n",
        "model = ConvAutoencoder().to(device)\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-5)\n",
        "print(model)\n",
        "\n",
        "# --- 11. Training loop (20 epochs) ---\n",
        "NUM_EPOCHS = 20\n",
        "train_losses, val_losses = [], []\n",
        "\n",
        "print(\"\\nüöÄ Training started...\")\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    # --- Training ---\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    for images in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{NUM_EPOCHS} [Train]\"):\n",
        "        images = images.to(device, non_blocking=True)\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, images)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    epoch_train_loss = running_loss / len(train_loader)\n",
        "    train_losses.append(epoch_train_loss)\n",
        "\n",
        "    # --- Validation ---\n",
        "    model.eval()\n",
        "    val_loss = 0.0\n",
        "    with torch.no_grad():\n",
        "        for images in val_loader:\n",
        "            images = images.to(device, non_blocking=True)\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, images)\n",
        "            val_loss += loss.item()\n",
        "    epoch_val_loss = val_loss / len(val_loader)\n",
        "    val_losses.append(epoch_val_loss)\n",
        "\n",
        "    print(f\"üìä Epoch [{epoch+1}/{NUM_EPOCHS}] | Train Loss: {epoch_train_loss:.6f} | Val Loss: {epoch_val_loss:.6f}\")\n",
        "\n",
        "print(\"‚úÖ Training finished!\")\n",
        "\n",
        "# --- 12. Plot loss curves ---\n",
        "plt.figure(figsize=(10,5))\n",
        "plt.plot(range(1, NUM_EPOCHS+1), train_losses, label=\"Train Loss\", marker='o')\n",
        "plt.plot(range(1, NUM_EPOCHS+1), val_losses, label=\"Val Loss\", marker='o')\n",
        "plt.xlabel(\"Epochs\"); plt.ylabel(\"MSE Loss\")\n",
        "plt.title(\"Training vs Validation Loss (Tesla T4 GPU)\")\n",
        "plt.legend(); plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "# --- 13. Reconstruction visualization ---\n",
        "model.eval()\n",
        "test_images = next(iter(test_loader)).to(device)\n",
        "with torch.no_grad():\n",
        "    reconstructed = model(test_images)\n",
        "\n",
        "plt.figure(figsize=(20,6))\n",
        "for i in range(5):\n",
        "    ax = plt.subplot(2, 5, i+1)\n",
        "    plt.imshow(test_images[i].cpu().permute(1,2,0))\n",
        "    plt.title(\"Original\"); plt.axis(\"off\")\n",
        "\n",
        "    ax = plt.subplot(2, 5, i+6)\n",
        "    plt.imshow(reconstructed[i].cpu().permute(1,2,0))\n",
        "    plt.title(\"Reconstructed\"); plt.axis(\"off\")\n",
        "plt.suptitle(\"Autoencoder Reconstructions (Tesla T4 GPU)\", fontsize=16)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "AvOb8iCJ82ro"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "id": "1sKTBrqBy7dV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 0. Fix: Mount Drive and Extract Dataset ZIP ---\n",
        "import os, zipfile, glob\n",
        "from google.colab import drive\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Path to ZIP file (with space handled correctly)\n",
        "zip_path = \"/content/drive/MyDrive/Dataset Conf/archive.zip\"\n",
        "extract_path = \"/content/local_dataset\"\n",
        "\n",
        "# Check if ZIP exists\n",
        "if not os.path.isfile(zip_path):\n",
        "    raise FileNotFoundError(f\"‚ùå ZIP not found at: {zip_path}\\nTip: Check spelling and spaces!\")\n",
        "\n",
        "print(f\"‚úÖ Found ZIP: {zip_path}\")\n",
        "\n",
        "# Extract only once\n",
        "if not os.path.exists(extract_path) or not os.listdir(extract_path):\n",
        "    print(\"‚è≥ Extracting dataset...\")\n",
        "    os.makedirs(extract_path, exist_ok=True)\n",
        "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "        zip_ref.extractall(extract_path)\n",
        "    print(\"‚úÖ Extraction done\")\n",
        "else:\n",
        "    print(\"‚ö° Already extracted, skipping extraction.\")\n",
        "\n",
        "# Collect only images (.jpg, .jpeg, .png)\n",
        "image_extensions = [\"*.jpg\", \"*.jpeg\", \"*.png\"]\n",
        "all_images = []\n",
        "for ext in image_extensions:\n",
        "    all_images.extend(glob.glob(os.path.join(extract_path, \"**\", ext), recursive=True))\n",
        "\n",
        "print(f\"üì∏ Total images found: {len(all_images):,}\")\n",
        "if all_images:\n",
        "    print(\"üëâ Example:\", all_images[0])\n"
      ],
      "metadata": {
        "id": "jDP5Pve-00GW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 1. Imports ---\n",
        "import os, glob, shutil, zipfile\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "!pip install lpips\n",
        "\n",
        "\n",
        "# Metrics\n",
        "from skimage.metrics import peak_signal_noise_ratio as psnr\n",
        "from skimage.metrics import structural_similarity as ssim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "try:\n",
        "    import lpips\n",
        "    lpips_alex = lpips.LPIPS(net='alex')\n",
        "    lpips_alex.cuda()\n",
        "except:\n",
        "    lpips_alex = None\n",
        "    print(\"‚ö†Ô∏è LPIPS not available, install with: pip install lpips\")\n",
        "\n",
        "# --- 2. Force GPU ---\n",
        "assert torch.cuda.is_available(), \"‚ùå No GPU found! Enable GPU runtime in Colab.\"\n",
        "device = torch.device(\"cuda\")\n",
        "print(f\"üî• Using device: {device}\")\n",
        "print(\"GPU Name:\", torch.cuda.get_device_name(0))\n",
        "\n",
        "# --- 3. Dataset paths ---\n",
        "zip_path = \"/content/drive/MyDrive/Dataset Conf/archive.zip\"\n",
        "extract_path = \"/content/local_dataset\"\n",
        "processed_path = \"/content/processed_dataset\"\n",
        "\n",
        "# --- 4. Extract dataset ---\n",
        "if not os.path.exists(extract_path):\n",
        "    print(\"‚è≥ Extracting dataset...\")\n",
        "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "        zip_ref.extractall(extract_path)\n",
        "    print(\"‚úÖ Extraction done\")\n",
        "else:\n",
        "    print(\"‚ö° Already extracted\")\n",
        "\n",
        "# --- 5. Collect images ---\n",
        "image_extensions = [\"*.jpg\", \"*.jpeg\", \"*.png\"]\n",
        "all_images = []\n",
        "for ext in image_extensions:\n",
        "    all_images.extend(glob.glob(os.path.join(extract_path, \"**\", ext), recursive=True))\n",
        "\n",
        "print(f\"üì∏ Total images collected: {len(all_images):,}\")\n",
        "if not all_images:\n",
        "    raise RuntimeError(\"‚ùå No images found!\")\n",
        "\n",
        "# --- 6. Split into train/val/test ---\n",
        "train_imgs, temp_imgs = train_test_split(all_images, test_size=0.2, random_state=42)\n",
        "val_imgs, test_imgs = train_test_split(temp_imgs, test_size=0.5, random_state=42)\n",
        "\n",
        "print(f\"üìÇ Train: {len(train_imgs):,}, Val: {len(val_imgs):,}, Test: {len(test_imgs):,}\")\n",
        "\n",
        "# --- 7. Organize dataset ---\n",
        "for split in [\"train\", \"val\", \"test\"]:\n",
        "    os.makedirs(os.path.join(processed_path, split), exist_ok=True)\n",
        "\n",
        "def copy_images(file_list, destination):\n",
        "    for f in file_list:\n",
        "        shutil.copy(f, destination)\n",
        "\n",
        "copy_images(train_imgs, os.path.join(processed_path, \"train\"))\n",
        "copy_images(val_imgs, os.path.join(processed_path, \"val\"))\n",
        "copy_images(test_imgs, os.path.join(processed_path, \"test\"))\n",
        "print(\"‚úÖ Dataset ready\")\n",
        "\n",
        "# --- 8. Dataset class ---\n",
        "class ImageDataset(Dataset):\n",
        "    def __init__(self, folder_path, transform=None):\n",
        "        self.image_paths = sorted(glob.glob(os.path.join(folder_path, \"*\")))\n",
        "        self.transform = transform\n",
        "    def __len__(self):\n",
        "        return len(self.image_paths)\n",
        "    def __getitem__(self, idx):\n",
        "        image = Image.open(self.image_paths[idx]).convert(\"RGB\")\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        return image\n",
        "\n",
        "# --- 9. Data loaders ---\n",
        "IMAGE_SIZE = (128, 128)\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize(IMAGE_SIZE),\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "\n",
        "train_dataset = ImageDataset(os.path.join(processed_path, \"train\"), transform=transform)\n",
        "val_dataset = ImageDataset(os.path.join(processed_path, \"val\"), transform=transform)\n",
        "test_dataset = ImageDataset(os.path.join(processed_path, \"test\"), transform=transform)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True,\n",
        "                          num_workers=4, pin_memory=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False,\n",
        "                        num_workers=4, pin_memory=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False,\n",
        "                         num_workers=4, pin_memory=True)\n",
        "\n",
        "print(f\"‚úÖ Train: {len(train_dataset)}, Val: {len(val_dataset)}, Test: {len(test_dataset)}\")\n",
        "\n",
        "# --- 10. Autoencoder model ---\n",
        "class ConvAutoencoder(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(ConvAutoencoder, self).__init__()\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Conv2d(3, 32, 3, stride=2, padding=1), nn.ReLU(),\n",
        "            nn.Conv2d(32, 64, 3, stride=2, padding=1), nn.ReLU(),\n",
        "            nn.Conv2d(64, 128, 3, stride=2, padding=1), nn.ReLU(),\n",
        "        )\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.ConvTranspose2d(128, 64, 3, stride=2, padding=1, output_padding=1), nn.ReLU(),\n",
        "            nn.ConvTranspose2d(64, 32, 3, stride=2, padding=1, output_padding=1), nn.ReLU(),\n",
        "            nn.ConvTranspose2d(32, 3, 3, stride=2, padding=1, output_padding=1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        return self.decoder(self.encoder(x))\n",
        "\n",
        "# --- 11. Init ---\n",
        "model = ConvAutoencoder().to(device)\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-5)\n",
        "\n",
        "# --- 12. Training ---\n",
        "NUM_EPOCHS = 20\n",
        "train_losses, val_losses = [], []\n",
        "\n",
        "print(\"\\nüöÄ Training started...\")\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    # Training\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    for images in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{NUM_EPOCHS} [Train]\"):\n",
        "        images = images.to(device, non_blocking=True)\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, images)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "    epoch_train_loss = running_loss / len(train_loader)\n",
        "    train_losses.append(epoch_train_loss)\n",
        "\n",
        "    # Validation\n",
        "    model.eval()\n",
        "    val_loss = 0.0\n",
        "    with torch.no_grad():\n",
        "        for images in val_loader:\n",
        "            images = images.to(device, non_blocking=True)\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, images)\n",
        "            val_loss += loss.item()\n",
        "    epoch_val_loss = val_loss / len(val_loader)\n",
        "    val_losses.append(epoch_val_loss)\n",
        "\n",
        "    print(f\"üìä Epoch [{epoch+1}/{NUM_EPOCHS}] | Train Loss: {epoch_train_loss:.6f} | Val Loss: {epoch_val_loss:.6f}\")\n",
        "\n",
        "    # Save models\n",
        "    torch.save(model.state_dict(), \"/content/autoencoder_last.pth\")\n",
        "    if epoch == 0 or epoch_val_loss <= min(val_losses):\n",
        "        torch.save(model.state_dict(), \"/content/autoencoder_best.pth\")\n",
        "\n",
        "print(\"‚úÖ Training finished!\")\n",
        "\n",
        "# --- 13. Loss curves ---\n",
        "plt.figure(figsize=(10,5))\n",
        "plt.plot(range(1, NUM_EPOCHS+1), train_losses, label=\"Train Loss\", marker='o')\n",
        "plt.plot(range(1, NUM_EPOCHS+1), val_losses, label=\"Val Loss\", marker='o')\n",
        "plt.xlabel(\"Epochs\"); plt.ylabel(\"MSE Loss\")\n",
        "plt.title(\"Training vs Validation Loss (Tesla T4 GPU)\")\n",
        "plt.legend(); plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "# --- 14. Evaluation on test set ---\n",
        "model.eval()\n",
        "test_loss, psnr_scores, ssim_scores, mae_scores, lpips_scores = 0.0, [], [], [], []\n",
        "with torch.no_grad():\n",
        "    for images in tqdm(test_loader, desc=\"Evaluating on Test Set\"):\n",
        "        images = images.to(device, non_blocking=True)\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, images)\n",
        "        test_loss += loss.item()\n",
        "\n",
        "        # Metrics\n",
        "        for i in range(images.size(0)):\n",
        "            orig = images[i].cpu().permute(1,2,0).numpy()\n",
        "            recon = outputs[i].cpu().permute(1,2,0).numpy()\n",
        "            psnr_scores.append(psnr(orig, recon, data_range=1.0))\n",
        "            ssim_scores.append(ssim(orig, recon, channel_axis=2, data_range=1.0))\n",
        "            mae_scores.append(np.mean(np.abs(orig - recon)))\n",
        "            if lpips_alex:\n",
        "                score = lpips_alex(images[i].unsqueeze(0), outputs[i].unsqueeze(0)).item()\n",
        "                lpips_scores.append(score)\n",
        "\n",
        "avg_test_loss = test_loss / len(test_loader)\n",
        "print(f\"\\n‚úÖ Test Results:\")\n",
        "print(f\"   Test Loss: {avg_test_loss:.6f}\")\n",
        "print(f\"   PSNR: {np.mean(psnr_scores):.3f}\")\n",
        "print(f\"   SSIM: {np.mean(ssim_scores):.3f}\")\n",
        "print(f\"   MAE: {np.mean(mae_scores):.6f}\")\n",
        "if lpips_scores:\n",
        "    print(f\"   LPIPS: {np.mean(lpips_scores):.6f}\")\n",
        "\n",
        "# --- 15. Error heatmaps (first 3 samples) ---\n",
        "test_images = next(iter(test_loader)).to(device)\n",
        "with torch.no_grad():\n",
        "    reconstructed = model(test_images)\n",
        "\n",
        "plt.figure(figsize=(15,6))\n",
        "for i in range(3):\n",
        "    orig = test_images[i].cpu().permute(1,2,0).numpy()\n",
        "    recon = reconstructed[i].cpu().permute(1,2,0).numpy()\n",
        "    error = np.abs(orig - recon)\n",
        "\n",
        "    plt.subplot(3,3,i*3+1); plt.imshow(orig); plt.title(\"Original\"); plt.axis(\"off\")\n",
        "    plt.subplot(3,3,i*3+2); plt.imshow(recon); plt.title(\"Reconstructed\"); plt.axis(\"off\")\n",
        "    plt.subplot(3,3,i*3+3); plt.imshow(error, cmap=\"hot\"); plt.title(\"Error Heatmap\"); plt.axis(\"off\")\n",
        "plt.suptitle(\"Reconstruction Quality on Test Images\", fontsize=16)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "M8qjzA6f3Ymj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from skimage.metrics import peak_signal_noise_ratio as psnr, structural_similarity as ssim\n",
        "import numpy as np\n",
        "\n",
        "# --- Initialize storage for curves ---\n",
        "psnr_epoch_scores, ssim_epoch_scores = [], []\n",
        "\n",
        "print(\"\\nüöÄ Training with PSNR/SSIM tracking...\")\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    # --- Training ---\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    for images in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{NUM_EPOCHS} [Train]\"):\n",
        "        images = images.to(device, non_blocking=True)\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, images)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "    epoch_train_loss = running_loss / len(train_loader)\n",
        "    train_losses.append(epoch_train_loss)\n",
        "\n",
        "    # --- Validation (with PSNR + SSIM) ---\n",
        "    model.eval()\n",
        "    val_loss, val_psnr, val_ssim, count = 0.0, 0.0, 0.0, 0\n",
        "    with torch.no_grad():\n",
        "        for images in val_loader:\n",
        "            images = images.to(device, non_blocking=True)\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, images)\n",
        "            val_loss += loss.item()\n",
        "\n",
        "            imgs_np = images.cpu().numpy().transpose(0,2,3,1)\n",
        "            outs_np = outputs.cpu().numpy().transpose(0,2,3,1)\n",
        "\n",
        "            for i in range(len(imgs_np)):\n",
        "                val_psnr += psnr(imgs_np[i], outs_np[i], data_range=1.0)\n",
        "                val_ssim += ssim(imgs_np[i], outs_np[i], channel_axis=-1, data_range=1.0)\n",
        "                count += 1\n",
        "\n",
        "    epoch_val_loss = val_loss / len(val_loader)\n",
        "    val_losses.append(epoch_val_loss)\n",
        "    psnr_epoch_scores.append(val_psnr / count)\n",
        "    ssim_epoch_scores.append(val_ssim / count)\n",
        "\n",
        "    print(f\"üìä Epoch [{epoch+1}/{NUM_EPOCHS}] | \"\n",
        "          f\"Train Loss: {epoch_train_loss:.6f} | \"\n",
        "          f\"Val Loss: {epoch_val_loss:.6f} | \"\n",
        "          f\"PSNR: {psnr_epoch_scores[-1]:.2f} | \"\n",
        "          f\"SSIM: {ssim_epoch_scores[-1]:.4f}\")\n",
        "\n",
        "print(\"‚úÖ Training + PSNR/SSIM tracking finished!\")\n",
        "\n",
        "# --- Plot PSNR & SSIM curves ---\n",
        "plt.figure(figsize=(10,5))\n",
        "plt.plot(range(1, NUM_EPOCHS+1), psnr_epoch_scores, label=\"PSNR\", marker='o')\n",
        "plt.plot(range(1, NUM_EPOCHS+1), ssim_epoch_scores, label=\"SSIM\", marker='s')\n",
        "plt.xlabel(\"Epochs\"); plt.ylabel(\"Score\")\n",
        "plt.title(\"Reconstruction Quality over Epochs (Tesla T4 GPU)\")\n",
        "plt.legend(); plt.grid(True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "Tk94CkCtbyzs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# --- Data extracted from your logs ---\n",
        "epochs = list(range(1, 21))\n",
        "\n",
        "train_losses = [0.000744,0.000267,0.000238,0.000223,0.000210,0.000205,0.000202,0.000199,0.000198,0.000196,\n",
        "                0.000195,0.000194,0.000193,0.000193,0.000192,0.000191,0.000191,0.000190,0.000190,0.000189]\n",
        "\n",
        "val_losses =   [0.000298,0.000219,0.000252,0.000188,0.000199,0.000183,0.000177,0.000179,0.000182,0.000183,\n",
        "                0.000176,0.000183,0.000210,0.000185,0.000178,0.000175,0.000178,0.000173,0.000183,0.000180]\n",
        "\n",
        "psnr_scores = [38.25,38.24,38.14,37.76,38.24,38.19,38.04,36.34,36.17,38.35,\n",
        "               38.08,38.20,37.62,36.39,37.84,37.40,38.45,37.87,37.94,38.22]\n",
        "\n",
        "ssim_scores = [0.9666,0.9659,0.9658,0.9665,0.9668,0.9677,0.9663,0.9552,0.9606,0.9676,\n",
        "               0.9672,0.9675,0.9670,0.9605,0.9660,0.9650,0.9666,0.9652,0.9675,0.9679]\n",
        "\n",
        "# --- 1. Train vs Validation Loss ---\n",
        "plt.figure(figsize=(8,5))\n",
        "plt.plot(epochs, train_losses, label=\"Train Loss\", marker='o')\n",
        "plt.plot(epochs, val_losses, label=\"Validation Loss\", marker='s')\n",
        "best_epoch = np.argmin(val_losses)+1\n",
        "plt.scatter(best_epoch, val_losses[best_epoch-1], color='red', s=80, label=f\"Best Val (Epoch {best_epoch})\")\n",
        "plt.xlabel(\"Epochs\", fontsize=12); plt.ylabel(\"MSE Loss\", fontsize=12)\n",
        "plt.title(\"Training vs Validation Loss\", fontsize=14, fontweight='bold')\n",
        "plt.legend(); plt.grid(True, linestyle='--', alpha=0.6)\n",
        "plt.show()\n",
        "\n",
        "# --- 2. PSNR across epochs ---\n",
        "plt.figure(figsize=(8,5))\n",
        "plt.plot(epochs, psnr_scores, label=\"PSNR\", marker='o', color='green')\n",
        "best_epoch = np.argmax(psnr_scores)+1\n",
        "plt.scatter(best_epoch, psnr_scores[best_epoch-1], color='red', s=80, label=f\"Best PSNR (Epoch {best_epoch})\")\n",
        "plt.xlabel(\"Epochs\", fontsize=12); plt.ylabel(\"PSNR (dB)\", fontsize=12)\n",
        "plt.title(\"Peak Signal-to-Noise Ratio (PSNR)\", fontsize=14, fontweight='bold')\n",
        "plt.legend(); plt.grid(True, linestyle='--', alpha=0.6)\n",
        "plt.show()\n",
        "\n",
        "# --- 3. SSIM across epochs ---\n",
        "plt.figure(figsize=(8,5))\n",
        "plt.plot(epochs, ssim_scores, label=\"SSIM\", marker='s', color='purple')\n",
        "best_epoch = np.argmax(ssim_scores)+1\n",
        "plt.scatter(best_epoch, ssim_scores[best_epoch-1], color='red', s=80, label=f\"Best SSIM (Epoch {best_epoch})\")\n",
        "plt.xlabel(\"Epochs\", fontsize=12); plt.ylabel(\"SSIM\", fontsize=12)\n",
        "plt.title(\"Structural Similarity Index (SSIM)\", fontsize=14, fontweight='bold')\n",
        "plt.legend(); plt.grid(True, linestyle='--', alpha=0.6)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "Ko8sYZb1K2VP"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}